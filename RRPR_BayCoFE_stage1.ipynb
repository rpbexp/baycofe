{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWKIoGdNlQde"
      },
      "outputs": [],
      "source": [
        "!pip install mlxtend\n",
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LCT5dWT0lWtm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import copy\n",
        "import random\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from skimage import filters\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import pickle\n",
        "from scipy.stats import kendalltau\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import norm, gaussian_kde\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.datasets import load_wine\n",
        "import itertools\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import VarianceThreshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QiF9610fmT9G"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "def remove_highly_correlated_features_pd(X, threshold):\n",
        "    # Convert X to a pandas DataFrame\n",
        "    # Calculate the correlation matrix\n",
        "    df = pd.DataFrame(X)\n",
        "    corr_matrix = df.corr().abs()\n",
        "    # Create a mask to remove the upper triangle of the correlation matrix\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "    # Set the upper triangle values to NaN\n",
        "    corr_matrix.mask(mask, inplace=True)\n",
        "\n",
        "    # Find the highly correlated features\n",
        "    cols_to_drop = [column for column in corr_matrix.columns if any(corr_matrix[column] > threshold)]\n",
        "\n",
        "    # Drop the highly correlated features from the DataFrame\n",
        "    reduced_df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "    # Convert the reduced DataFrame back to numpy array\n",
        "    X_reduced = reduced_df.to_numpy()\n",
        "\n",
        "    return X_reduced, cols_to_drop\n",
        "\n",
        "\n",
        "def calculate_entropy(data):\n",
        "    if np.var(data) == 0:\n",
        "        return 0\n",
        "\n",
        "    scipy_kernel = gaussian_kde(data)\n",
        "\n",
        "    #  We calculate the bandwidth for later use\n",
        "    optimal_bandwidth = scipy_kernel.factor * np.std(data)\n",
        "\n",
        "    # Calculate KDE for the entire dataset\n",
        "    kde = gaussian_kde(data, bw_method=optimal_bandwidth)\n",
        "\n",
        "    # Create a range of values to represent the KDE\n",
        "    x = np.linspace(np.min(data), np.max(data), 1000)\n",
        "\n",
        "    # Evaluate the density at each point in the range\n",
        "    density = kde(x)\n",
        "\n",
        "    # Normalize the density function\n",
        "    normalized_density = density / np.sum(density * (x[1] - x[0]))\n",
        "\n",
        "    # Calculate the probabilities of positive and negative values\n",
        "    positive_probability = np.sum(normalized_density[x >= 0] * (x[1] - x[0]))\n",
        "    negative_probability = np.sum(normalized_density[x < 0] * (x[1] - x[0]))\n",
        "\n",
        "    if positive_probability == 0 or negative_probability == 0:\n",
        "        sign_entropy = 0\n",
        "    else:\n",
        "        sign_entropy = -positive_probability * np.log2(positive_probability) \\\n",
        "                       - negative_probability * np.log2(negative_probability)\n",
        "\n",
        "    return sign_entropy\n",
        "\n",
        "\n",
        "def calculate_entropies(result_matrix):\n",
        "    sign_entropies = []\n",
        "    for column in range(result_matrix.shape[1]):\n",
        "        data = result_matrix[:, column]\n",
        "        sign_entropy = calculate_entropy(data)\n",
        "        sign_entropies.append(sign_entropy)\n",
        "\n",
        "    sign_entropies = np.array(sign_entropies)\n",
        "\n",
        "    return sign_entropies\n",
        "\n",
        "\n",
        "def get_unstable_features(X, y, model, bs_indices, num_bootstraps=None):\n",
        "    coeffs_bs = []\n",
        "    if num_bootstraps != None:\n",
        "      print(\"bootstraps generated\")\n",
        "      bs_indices = []\n",
        "      for i in np.arange(0, num_bootstraps, step=1):\n",
        "        max_bs_range = X.shape[0]\n",
        "        indx_bs = random.choices(range(max_bs_range), k=max_bs_range)\n",
        "        bs_indices.append(indx_bs)\n",
        "\n",
        "    for i in range(len(bs_indices)):\n",
        "      indices = bs_indices[i]\n",
        "      X_sample, y_sample = X[indices], y[indices]\n",
        "      model.fit(X_sample, y_sample)\n",
        "      coeffs_bs.append(model.coef_)\n",
        "    coeffs_bs = np.array(coeffs_bs)\n",
        "\n",
        "    sign_entropies = []\n",
        "    for column in range(coeffs_bs.shape[1]):\n",
        "        data = coeffs_bs[:, column]\n",
        "        sign_entropy = calculate_entropy(data)\n",
        "        sign_entropies.append(sign_entropy)\n",
        "\n",
        "    num_predictors = len(sign_entropies)\n",
        "    sign_entropies = np.array(sign_entropies)\n",
        "    av_sign_entropy = np.mean(sign_entropies)\n",
        "    ratio_zero_entropy = np.count_nonzero(sign_entropies == 0) / (sign_entropies.shape[0])\n",
        "\n",
        "    non_zero_indices = np.where(sign_entropies != 0)[0]\n",
        "    zero_ent_indices = np.where(sign_entropies == 0)[0]\n",
        "\n",
        "\n",
        "\n",
        "    return non_zero_indices, zero_ent_indices, sign_entropies, coeffs_bs\n",
        "\n",
        "\n",
        "def evaluate_feature_selection_method(feature_selector, X_train, y_train, X_test, y_test,eval_indices_bs,\n",
        "                                      selector_params={}, model_params={}):\n",
        "    \"\"\"\n",
        "    Evaluate a given feature selection method on training and test data.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_selector: function, a feature selection method that returns selected feature indices.\n",
        "    - X_train, y_train, X_test, y_test: train and test datasets.\n",
        "    - selector_params: dict, parameters for the feature selector.\n",
        "    - model_params: dict, parameters for the model.\n",
        "    - i: iteration index, useful for storing results in dictionaries.\n",
        "\n",
        "    Returns:\n",
        "    - rmse: Root Mean Squared Error using the selected features.\n",
        "    - sign_entropies: Sign entropies of the selected features.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply feature selection\n",
        "    selected_indices = feature_selector(X_train, y_train, **selector_params)\n",
        "    # print(\"Selected Features: \", selected_indices)\n",
        "\n",
        "    # Subset data using selected features\n",
        "    X_train_selected = X_train[:, selected_indices]\n",
        "    X_test_selected = X_test[:, selected_indices]\n",
        "\n",
        "    # Fit and predict using the model\n",
        "    model = selector_params['estimator']\n",
        "    model.fit(X_train_selected, y_train)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    evaluation_model = selector_params['evaluator']\n",
        "\n",
        "    _, _, sign_entropies, coeffs_bs = get_unstable_features(X_test_selected, y_test, evaluation_model, bs_indices=eval_indices_bs)\n",
        "\n",
        "    return selected_indices, rmse, sign_entropies, coeffs_bs\n",
        "\n",
        "\n",
        "def sefe_select_features(X_train, y_train, num_bootstrap=1000, slack=0.0, **kwargs):\n",
        "    model = kwargs['estimator']\n",
        "    num_iter = 10\n",
        "    tolerance_limit = 1\n",
        "    tolerance_cur=0\n",
        "    final_coeffs = np.zeros((num_bootstrap, X_train.shape[1]))\n",
        "\n",
        "    train_mat_sel_idx = np.zeros(X_train.shape[1])\n",
        "    for iter in range(num_iter):\n",
        "        zero_indices = np.where(train_mat_sel_idx == 0)[0] # get only those indices which had zero entropy from previous run\n",
        "\n",
        "        if len(zero_indices)==0:\n",
        "            print(\"No feature of zero entropy\")\n",
        "            break\n",
        "\n",
        "        coeffs_bs = []\n",
        "        for i in range(num_bootstrap):\n",
        "            indices_bs = random.choices(range(X_train.shape[0]), k=X_train.shape[0])\n",
        "            X_sample, y_sample = X_train[indices_bs][:, zero_indices], y_train[indices_bs]\n",
        "            model.fit(X_sample, y_sample)\n",
        "            coeffs_bs.append(model.coef_)\n",
        "\n",
        "        coeffs_bs = np.array(coeffs_bs)\n",
        "\n",
        "        sign_entropies = []\n",
        "        for column in range(coeffs_bs.shape[1]):\n",
        "            data = coeffs_bs[:, column]\n",
        "            sign_entropy = calculate_entropy(data)\n",
        "            sign_entropies.append(sign_entropy)\n",
        "\n",
        "        sign_entropies = np.array(sign_entropies)\n",
        "\n",
        "        non_zero_indices = np.where(sign_entropies != 0)[0]\n",
        "        zero_ent_indices = np.where(sign_entropies == 0)[0]\n",
        "\n",
        "        #non_zero_indices = np.where(sign_entropies > (0+slack))[0]\n",
        "        #zero_ent_indices = np.where(sign_entropies <= (0+slack))[0]\n",
        "\n",
        "        original_0_indices = np.where(train_mat_sel_idx == 0)[0]\n",
        "        mapped_non0_indices = original_0_indices[non_zero_indices]\n",
        "\n",
        "        if not np.size(mapped_non0_indices) == 0:\n",
        "            train_mat_sel_idx[mapped_non0_indices] = 1\n",
        "            tolerance_cur = 0\n",
        "        else:\n",
        "            if tolerance_cur == 0:\n",
        "                final_coeffs = coeffs_bs\n",
        "                tolerance_cur = tolerance_cur + 1\n",
        "            else:\n",
        "                if tolerance_cur < tolerance_limit:\n",
        "                    tolerance_cur = tolerance_cur + 1  # re-evaluating n times after a good run\n",
        "                else:\n",
        "                    break  # terminate the feature elimination process\n",
        "\n",
        "    top_idx = np.array(np.where(train_mat_sel_idx==0)[0])\n",
        "    if (len(top_idx)) == 0:\n",
        "        top_idx = np.arange(0, len(train_mat_sel_idx), step=1)\n",
        "\n",
        "    return top_idx\n",
        "\n",
        "\n",
        "def boruta_select_features_old(X_train, y_train, **kwargs):\n",
        "    k = kwargs.get('n_features_to_select', X_train.shape[1])  # default to all features if not provided\n",
        "    model = kwargs['estimator']\n",
        "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=42)\n",
        "    feat_selector.fit(X_train, y_train)\n",
        "    # Get ranking of the features\n",
        "    ranking = feat_selector.ranking_\n",
        "    # Sort features by ranking\n",
        "    top_k_idx = np.argsort(ranking)[:k]\n",
        "    return top_k_idx\n",
        "\n",
        "\n",
        "\n",
        "def rfe_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    max_k = X_train.shape[1]  # Maximum number of features\n",
        "\n",
        "    param_grid = {'n_features_to_select': range(1, max_k + 1)}  # Define range of k values\n",
        "    selector = RFE(estimator=estimator)\n",
        "    grid_search = GridSearchCV(selector, param_grid, cv=5)  # 5-fold cross-validation\n",
        "    grid_search.fit(X_train, y_train)  # X_train and y_train are your training data\n",
        "\n",
        "    best_k = grid_search.best_params_['n_features_to_select']\n",
        "    selected_features = [i for i, selected in enumerate(grid_search.best_estimator_.support_) if selected]\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "\n",
        "def forward_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    n_features_to_select = kwargs.get('n_features_to_select', X_train.shape[1])\n",
        "\n",
        "    selector = SequentialFeatureSelector(estimator=estimator, k_features=(1, X_train.shape[1]), cv=5, forward=True)\n",
        "    selector.fit(X_train, y_train)\n",
        "    return selector.k_feature_idx_\n",
        "\n",
        "\n",
        "def sbs_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    n_features_to_select = kwargs.get('n_features_to_select', X_train.shape[1])\n",
        "    selector = SequentialFeatureSelector(estimator=estimator, k_features=(1, X_train.shape[1]), cv=5, forward=False)\n",
        "    selector.fit(X_train, y_train)\n",
        "    return selector.k_feature_idx_\n",
        "\n",
        "\n",
        "def bidirectional_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    n_features_to_select = kwargs.get('n_features_to_select', X_train.shape[1])\n",
        "    selector = SequentialFeatureSelector(estimator=estimator, k_features=(1, X_train.shape[1]), cv=5, forward=True,\n",
        "                                         floating=True)\n",
        "    selector.fit(X_train, y_train)\n",
        "    return selector.k_feature_idx_\n",
        "\n",
        "\n",
        "def split_data_into_parts(X, y, K):\n",
        "    \"\"\"\n",
        "    Split the feature matrix X and target values y into K approximately equal-sized parts.\n",
        "\n",
        "    Parameters:\n",
        "        X (array-like): The feature matrix.\n",
        "        y (array-like): The target values.\n",
        "        K (int): The number of parts to split the data into.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: Each tuple contains the feature matrix and target values for one part.\n",
        "    \"\"\"\n",
        "    num_samples = X.shape[0]\n",
        "    part_size = num_samples // K\n",
        "    parts = []\n",
        "\n",
        "    # Shuffle the data randomly\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    X_shuffled = X[indices]\n",
        "    y_shuffled = y[indices]\n",
        "\n",
        "    # Split the shuffled data into K parts\n",
        "    for i in range(K):\n",
        "        start_idx = i * part_size\n",
        "        end_idx = start_idx + part_size if i < K - 1 else num_samples\n",
        "        X_part = X_shuffled[start_idx:end_idx]\n",
        "        y_part = y_shuffled[start_idx:end_idx]\n",
        "        parts.append((X_part, y_part))\n",
        "\n",
        "    return parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e2DjzwElpHnN"
      },
      "outputs": [],
      "source": [
        "#### BAY COFE SPECIFIC\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import pandas as pd\n",
        "from scipy.stats import norm, gaussian_kde\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import pickle\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy.stats import norm\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zA_GC-cp-Qoq"
      },
      "outputs": [],
      "source": [
        "def calculate_entropy_dist(mu, sigma):\n",
        "    #p_positive = 1 - norm.cdf(-mu / sigma)\n",
        "    #p_negative = norm.cdf(-mu / sigma)\n",
        "    p_negative = norm.cdf(0, mu, sigma)\n",
        "    p_positive = 1 - p_negative\n",
        "\n",
        "    # Avoiding log(0) issue by adding a small epsilon where probabilities are 0\n",
        "    epsilon = 1e-10\n",
        "    entropy = -(p_positive * np.log2(p_positive + epsilon) + p_negative * np.log2(p_negative + epsilon))\n",
        "\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GGLjGJ7M-VQr"
      },
      "outputs": [],
      "source": [
        "def baycofe_select_features(X_train, y_train, slack=0.001, **kwargs): #housing: 0.001 energy: 0.025 superconductivity: 0.005\n",
        "    model = kwargs['estimator']\n",
        "    num_iter = 10\n",
        "    tolerance_limit = 1\n",
        "    tolerance_cur=0\n",
        "    #final_coeffs = np.zeros((num_bootstrap, X_train.shape[1]))\n",
        "\n",
        "    train_mat_sel_idx = np.zeros(X_train.shape[1])\n",
        "    for iter in range(num_iter):\n",
        "        zero_indices = np.where(train_mat_sel_idx == 0)[0] # get only those indices which had zero entropy from previous run\n",
        "\n",
        "        if len(zero_indices)==0:\n",
        "            print(\"No feature of zero entropy\")\n",
        "            break\n",
        "\n",
        "        X_selected = X_train[:, zero_indices]\n",
        "        model.fit(X_selected, y_train)\n",
        "\n",
        "        beta_means = model.coef_\n",
        "        beta_stds = np.sqrt(np.diag(model.sigma_))\n",
        "\n",
        "        sign_entropies = []\n",
        "        for beta_mean, beta_std in zip(beta_means, beta_stds):\n",
        "            sign_entropies.append(calculate_entropy_dist(beta_mean, beta_std))\n",
        "\n",
        "        sign_entropies = np.array(sign_entropies)\n",
        "        non_zero_indices = np.where(sign_entropies > slack)[0]\n",
        "        zero_ent_indices = np.where(sign_entropies <= slack)[0]\n",
        "\n",
        "        #non_zero_indices = np.where(sign_entropies > (0+slack))[0]\n",
        "        #zero_ent_indices = np.where(sign_entropies <= (0+slack))[0]\n",
        "\n",
        "        original_0_indices = np.where(train_mat_sel_idx == 0)[0]\n",
        "        mapped_non0_indices = original_0_indices[non_zero_indices]\n",
        "\n",
        "        if not np.size(mapped_non0_indices) == 0:\n",
        "            train_mat_sel_idx[mapped_non0_indices] = 1\n",
        "            tolerance_cur = 0\n",
        "        else:\n",
        "            if tolerance_cur == 0:\n",
        "                #final_coeffs = coeffs_bs\n",
        "                tolerance_cur = tolerance_cur + 1\n",
        "            else:\n",
        "                if tolerance_cur < tolerance_limit:\n",
        "                    tolerance_cur = tolerance_cur + 1  # re-evaluating n times after a good run\n",
        "                else:\n",
        "                    break  # terminate the feature elimination process\n",
        "\n",
        "    top_idx = np.array(np.where(train_mat_sel_idx==0)[0])\n",
        "    if (len(top_idx)) == 0:\n",
        "        top_idx = np.arange(0, len(train_mat_sel_idx), step=1)\n",
        "\n",
        "    return top_idx\n",
        "\n",
        "def remove_highly_correlated_features_pd(X, threshold):\n",
        "    # Convert X to a pandas DataFrame\n",
        "    # Calculate the correlation matrix\n",
        "    df = pd.DataFrame(X)\n",
        "    corr_matrix = df.corr().abs()\n",
        "    # Create a mask to remove the upper triangle of the correlation matrix\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "    # Set the upper triangle values to NaN\n",
        "    corr_matrix.mask(mask, inplace=True)\n",
        "\n",
        "    # Find the highly correlated features\n",
        "    cols_to_drop = [column for column in corr_matrix.columns if any(corr_matrix[column] > threshold)]\n",
        "\n",
        "    # Drop the highly correlated features from the DataFrame\n",
        "    reduced_df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "    # Convert the reduced DataFrame back to numpy array\n",
        "    X_reduced = reduced_df.to_numpy()\n",
        "\n",
        "    return X_reduced, cols_to_drop\n",
        "\n",
        "\n",
        "def calculate_entropy(data):\n",
        "    if np.var(data) == 0:\n",
        "        return 0\n",
        "\n",
        "    scipy_kernel = gaussian_kde(data)\n",
        "\n",
        "    #  We calculate the bandwidth for later use\n",
        "    optimal_bandwidth = scipy_kernel.factor * np.std(data)\n",
        "\n",
        "    # Calculate KDE for the entire dataset\n",
        "    kde = gaussian_kde(data, bw_method=optimal_bandwidth)\n",
        "\n",
        "    # Create a range of values to represent the KDE\n",
        "    x = np.linspace(np.min(data), np.max(data), 1000)\n",
        "\n",
        "    # Evaluate the density at each point in the range\n",
        "    density = kde(x)\n",
        "\n",
        "    # Normalize the density function\n",
        "    normalized_density = density / np.sum(density * (x[1] - x[0]))\n",
        "\n",
        "    # Calculate the probabilities of positive and negative values\n",
        "    positive_probability = np.sum(normalized_density[x >= 0] * (x[1] - x[0]))\n",
        "    negative_probability = np.sum(normalized_density[x < 0] * (x[1] - x[0]))\n",
        "\n",
        "    if positive_probability == 0 or negative_probability == 0:\n",
        "        sign_entropy = 0\n",
        "    else:\n",
        "        sign_entropy = -positive_probability * np.log2(positive_probability) \\\n",
        "                       - negative_probability * np.log2(negative_probability)\n",
        "\n",
        "    return sign_entropy\n",
        "\n",
        "\n",
        "def calculate_entropies(result_matrix):\n",
        "    sign_entropies = []\n",
        "    for column in range(result_matrix.shape[1]):\n",
        "        data = result_matrix[:, column]\n",
        "        sign_entropy = calculate_entropy(data)\n",
        "        sign_entropies.append(sign_entropy)\n",
        "\n",
        "    sign_entropies = np.array(sign_entropies)\n",
        "\n",
        "    return sign_entropies\n",
        "\n",
        "\n",
        "\n",
        "def get_unstable_features(X, y, model, bs_indices, num_bootstraps=None):\n",
        "    coeffs_bs = []\n",
        "    if num_bootstraps != None:\n",
        "      print(\"bootstraps generated\")\n",
        "      bs_indices = []\n",
        "      for i in np.arange(0, num_bootstraps, step=1):\n",
        "        max_bs_range = X.shape[0]\n",
        "        indx_bs = random.choices(range(max_bs_range), k=max_bs_range)\n",
        "        bs_indices.append(indx_bs)\n",
        "\n",
        "    for i in range(len(bs_indices)):\n",
        "      indices = bs_indices[i]\n",
        "      X_sample, y_sample = X[indices], y[indices]\n",
        "      model.fit(X_sample, y_sample)\n",
        "      coeffs_bs.append(model.coef_)\n",
        "    coeffs_bs = np.array(coeffs_bs)\n",
        "\n",
        "    sign_entropies = []\n",
        "    for column in range(coeffs_bs.shape[1]):\n",
        "        data = coeffs_bs[:, column]\n",
        "        sign_entropy = calculate_entropy(data)\n",
        "        sign_entropies.append(sign_entropy)\n",
        "\n",
        "    num_predictors = len(sign_entropies)\n",
        "    sign_entropies = np.array(sign_entropies)\n",
        "    av_sign_entropy = np.mean(sign_entropies)\n",
        "    ratio_zero_entropy = np.count_nonzero(sign_entropies == 0) / (sign_entropies.shape[0])\n",
        "\n",
        "    non_zero_indices = np.where(sign_entropies != 0)[0]\n",
        "    zero_ent_indices = np.where(sign_entropies == 0)[0]\n",
        "\n",
        "    return non_zero_indices, zero_ent_indices, sign_entropies, coeffs_bs\n",
        "\n",
        "\n",
        "def evaluate_feature_selection_method(feature_selector, X_train, y_train, X_test, y_test, eval_indices_bs,\n",
        "                                      selector_params={}, model_params={}):\n",
        "    selected_indices = feature_selector(X_train, y_train, **selector_params)\n",
        "\n",
        "    X_train_selected = X_train[:, selected_indices]\n",
        "    X_test_selected = X_test[:, selected_indices]\n",
        "    model = selector_params['estimator']\n",
        "    model.fit(X_train_selected, y_train)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    _, _, sign_entropies, coeffs_bs = get_unstable_features(X_test_selected, y_test, model, eval_indices_bs)\n",
        "    return selected_indices, rmse, sign_entropies, coeffs_bs\n",
        "\n",
        "def split_data_into_parts(X, y, K):\n",
        "    \"\"\"\n",
        "    Split the feature matrix X and target values y into K approximately equal-sized parts.\n",
        "\n",
        "    Parameters:\n",
        "        X (array-like): The feature matrix.\n",
        "        y (array-like): The target values.\n",
        "        K (int): The number of parts to split the data into.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: Each tuple contains the feature matrix and target values for one part.\n",
        "    \"\"\"\n",
        "    num_samples = X.shape[0]\n",
        "    part_size = num_samples // K\n",
        "    parts = []\n",
        "\n",
        "    # Shuffle the data randomly\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    X_shuffled = X[indices]\n",
        "    y_shuffled = y[indices]\n",
        "\n",
        "    # Split the shuffled data into K parts\n",
        "    for i in range(K):\n",
        "        start_idx = i * part_size\n",
        "        end_idx = start_idx + part_size if i < K - 1 else num_samples\n",
        "        X_part = X_shuffled[start_idx:end_idx]\n",
        "        y_part = y_shuffled[start_idx:end_idx]\n",
        "        parts.append((X_part, y_part))\n",
        "\n",
        "    return parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c2Tvd-EqJ4xO"
      },
      "outputs": [],
      "source": [
        "def get_energy_XY():\n",
        "  # Load the dataset into a pandas DataFrame\n",
        "  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n",
        "  df = pd.read_csv(url)\n",
        "  X = df.drop(columns=['Appliances', 'date', 'lights'])  # Remove 'Appliances', 'date', and 'lights' columns as features\n",
        "  y = df['Appliances'].values  # Target variable is 'Appliances'\n",
        "\n",
        "  threshold = 0.80\n",
        "  X_reduced, removed_feature_names = remove_highly_correlated_features_pd(X,threshold)\n",
        "  X = X_reduced\n",
        "  print(removed_feature_names)\n",
        "\n",
        "  print(\"Shape of X:\", X.shape)\n",
        "  print(\"Shape of y:\", y.shape)\n",
        "  print(X)\n",
        "  # Scale features\n",
        "  X_min = X.min(axis=0)\n",
        "  X_max = X.max(axis=0)\n",
        "  X_scaled = (X - X_min) / (X_max - X_min)\n",
        "  X = X_scaled\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7qvJR7ZPd8y1"
      },
      "outputs": [],
      "source": [
        "#download the data from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data and out train.csv on the same dir as this notebook\n",
        "\n",
        "def get_housing_XY():\n",
        "  df = pd.read_csv('train.csv', sep=',')\n",
        "  df.head()\n",
        "  # List of columns representing categorical variables\n",
        "  categorical_columns = ['Id','MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
        "                        'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
        "                        'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n",
        "                        'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
        "                        'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
        "                        'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n",
        "                        'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
        "                        'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType',\n",
        "                        'SaleCondition']\n",
        "  # Remove categorical columns from the DataFrame\n",
        "  numeric_df = df.drop(columns=categorical_columns, errors='ignore')\n",
        "  numeric_df = numeric_df.interpolate()\n",
        "  X = numeric_df.iloc[:, :-1]  # All columns except the last one\n",
        "  y = numeric_df.iloc[:, -1].values   # Last column\n",
        "\n",
        "  threshold = 0.80\n",
        "  X_reduced, removed_feature_names = remove_highly_correlated_features_pd(X,threshold)\n",
        "  X = X_reduced\n",
        "  print(removed_feature_names)\n",
        "\n",
        "  print(\"Shape of X:\", X.shape)\n",
        "  print(\"Shape of y:\", y.shape)\n",
        "  print(X)\n",
        "  # Scale features\n",
        "  X_min = X.min(axis=0)\n",
        "  X_max = X.max(axis=0)\n",
        "  X_scaled = (X - X_min) / (X_max - X_min)\n",
        "  X = X_scaled\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "POP5qWV3q1Qj"
      },
      "outputs": [],
      "source": [
        "def get_superconductivity_XY():\n",
        "  superconductivty_data = fetch_ucirepo(id=464)\n",
        "\n",
        "  # data (as pandas dataframes)\n",
        "  X = superconductivty_data.data.features\n",
        "  y = superconductivty_data.data.targets\n",
        "\n",
        "  threshold = 0.80\n",
        "  X_reduced, removed_feature_names = remove_highly_correlated_features_pd(X,threshold)\n",
        "  X = X_reduced\n",
        "  print(removed_feature_names)\n",
        "\n",
        "  print(\"Shape of X:\", X.shape)\n",
        "  print(\"Shape of y:\", y.shape)\n",
        "  # Scale features\n",
        "  X_min = X.min(axis=0)\n",
        "  X_max = X.max(axis=0)\n",
        "  X_scaled = (X - X_min) / (X_max - X_min)\n",
        "  X = X_scaled\n",
        "  print(X)\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-AUAktFARYhC"
      },
      "outputs": [],
      "source": [
        "def get_XY(dataset_name):\n",
        "  if dataset_name == 'superconductivity':\n",
        "    X, y = get_superconductivity_XY()\n",
        "  elif dataset_name == 'housing':\n",
        "    X, y = get_housing_XY()\n",
        "  elif dataset_name == 'energy':\n",
        "    X, y = get_energy_XY()\n",
        "  else:\n",
        "    print(\"Wrong Dataset Name\")\n",
        "    X = y = None\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kabOvvtBgp8H"
      },
      "outputs": [],
      "source": [
        "### ALL FEATURE SELECTION\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "k = 5\n",
        "num_folds = 5\n",
        "alpha_val = 0.1\n",
        "num_runs = 10 #change if required\n",
        "num_bootstraps = 1000 #change if required\n",
        "csem_pert_range = 0.2\n",
        "num_perturb = 1000\n",
        "csem_iter = 10\n",
        "save_dir = '/content/' # makedir to store results\n",
        "\n",
        "model_params = {\n",
        "    'alpha': 1\n",
        "}\n",
        "\n",
        "\n",
        "csem_params = {\n",
        "    'pert_range': csem_pert_range,\n",
        "    'n_perturb': num_perturb,\n",
        "    'n_iter': csem_iter\n",
        "}\n",
        "\n",
        "model_params = {\n",
        "    'alpha': 1\n",
        "}\n",
        "\n",
        "dataset_names = ['housing', 'superconductivity', 'energy']#['superconductivity'] #, 'housing', 'energy']\n",
        "\n",
        "lambda_inits = [0, 0.1, 1, 5]\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "  X, y = get_XY(dataset_name)\n",
        "  for lambda_init in lambda_inits:\n",
        "    pkl_filename = f'{save_dir}{dataset_name}_bayCoFE_R_{str(lambda_init*10)}_numfolds_CVk_{num_folds}.pkl'\n",
        "\n",
        "    if os.path.exists(pkl_filename):\n",
        "      print(f\"{pkl_filename} Exists... skipping...\")\n",
        "      continue\n",
        "\n",
        "    if lambda_init== 0:\n",
        "      model = BayesianRidge(lambda_init=1e-10, fit_intercept=True, compute_score=True)\n",
        "    else:\n",
        "      model = BayesianRidge(lambda_init=lambda_init, fit_intercept=True, compute_score=True)\n",
        "\n",
        "    selected_features_dict = {}\n",
        "    rmse_dict = {}\n",
        "    entropy_dict = {}\n",
        "    coeffs_dict = {}\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        print(f\"#############{dataset_name}####{lambda_init}############# {i}\")\n",
        "        data_parts = split_data_into_parts(X, y, num_folds)\n",
        "        part_permutations = itertools.permutations(data_parts, 2)\n",
        "        for perm in part_permutations:\n",
        "            train_data, test_data = perm\n",
        "            X_train, y_train = train_data\n",
        "            X_test, y_test = test_data\n",
        "\n",
        "            indices_bs = [random.choices(range(X_test.shape[0]), k=X_test.shape[0]) for _ in range(num_bootstraps)]\n",
        "\n",
        "            # baycofe feature selection\n",
        "            selected_indices_sem, rmse_sem, sign_entropies_sem, coeffs_bs_sem = \\\n",
        "                evaluate_feature_selection_method(baycofe_select_features, X_train, y_train, X_test, y_test, indices_bs,\n",
        "                                                  selector_params={'estimator': model,\n",
        "                                                                  'evaluator': model},\n",
        "                                                  model_params={'sigma_threshold': 1.5, 'iter_max': 10, 'n_convergence_iters': 3})\n",
        "\n",
        "            selected_features_dict.setdefault('baycofe', []).append(selected_indices_sem)\n",
        "            rmse_dict.setdefault('baycofe', []).append(rmse_sem)\n",
        "            entropy_dict.setdefault('baycofe', []).append(sign_entropies_sem)\n",
        "            coeffs_dict.setdefault('baycofe', []).append(coeffs_bs_sem)\n",
        "            print(\"Entropy BayCOFE:\", np.mean(sign_entropies_sem))\n",
        "            print(\"BayCOFE selected features:\", selected_indices_sem)\n",
        "            print(\"RMSE BayCOFE:\", rmse_sem)\n",
        "\n",
        "            ###cofe\n",
        "            selected_indices_sefe, rmse_sefe, sign_entropies_sefe, coeffs_bs_sefe = \\\n",
        "                evaluate_feature_selection_method(sefe_select_features, X_train, y_train,\n",
        "                                                  X_test, y_test,\n",
        "                                                  selector_params={'estimator': model,\n",
        "                                                                  'evaluator': model,\n",
        "                                                                  'n_features_to_select': k},\n",
        "                                                  model_params=model_params,\n",
        "                                                  eval_indices_bs=indices_bs)\n",
        "            k = len(selected_indices_sefe)\n",
        "\n",
        "            selected_features_dict.setdefault('cofe', []).append(selected_indices_sefe)\n",
        "            rmse_dict.setdefault('cofe', []).append(rmse_sefe)\n",
        "            entropy_dict.setdefault('cofe', []).append(sign_entropies_sefe)\n",
        "            coeffs_dict.setdefault('cofe', []).append(coeffs_bs_sefe)\n",
        "            print(\"Entropy COFE:\", np.mean(sign_entropies_sefe))\n",
        "            print(\"COFE selected features:\", selected_indices_sefe)\n",
        "            print(\"rmse COFE:\", rmse_sefe)\n",
        "\n",
        "            # RFE\n",
        "            selected_indices_rfe, rmse_rfe, sign_entropies_rfe, coeffs_bs_rfe = evaluate_feature_selection_method(\n",
        "                rfe_select_features, X_train,\n",
        "                y_train, X_test, y_test, selector_params={'estimator': model,\n",
        "                                                          'evaluator': model,\n",
        "                                                          'n_features_to_select': k,\n",
        "                                                          'n_iter': csem_params['n_iter']},\n",
        "                model_params=model_params,\n",
        "                eval_indices_bs=indices_bs)\n",
        "            # rmse_dict[f'rfe_{i}'] = rmse_rfe\n",
        "            # entropy_dict[f'rfe_{i}'] = sign_entropies_rfe\n",
        "            # selected_features_dict[f'rfe_{i}'] = selected_indices_rfe\n",
        "            selected_features_dict.setdefault('rfe', []).append(selected_indices_rfe)\n",
        "            rmse_dict.setdefault('rfe', []).append(rmse_rfe)\n",
        "            entropy_dict.setdefault('rfe', []).append(sign_entropies_rfe)\n",
        "            coeffs_dict.setdefault('rfe', []).append(coeffs_bs_rfe)\n",
        "            #print(\"RFE num features:\", selected_indices_rfe)\n",
        "            print(\"Entropy rfe:\", np.mean(sign_entropies_rfe))\n",
        "            print(\"rmse rfe:\", rmse_rfe)\n",
        "\n",
        "\n",
        "            ##SFS\n",
        "            selected_indices_sfs, rmse_sfs, sign_entropies_sfs, coeffs_bs_sfs = evaluate_feature_selection_method(\n",
        "                forward_select_features, X_train, y_train, X_test, y_test,\n",
        "                selector_params={'estimator': model, 'evaluator': model,\n",
        "                                'n_features_to_select': k},\n",
        "                model_params=model_params, eval_indices_bs=indices_bs)\n",
        "            # selected_features_dict[f'sfs_{i}'] = selected_indices_sfs\n",
        "            # rmse_dict[f'sfs_{i}'] = rmse_sfs\n",
        "            # entropy_dict[f'sfs_{i}'] = sign_entropies_sfs\n",
        "            selected_features_dict.setdefault('sfs', []).append(selected_indices_sfs)\n",
        "            rmse_dict.setdefault('sfs', []).append(rmse_sfs)\n",
        "            entropy_dict.setdefault('sfs', []).append(sign_entropies_sfs)\n",
        "            coeffs_dict.setdefault('sfs', []).append(coeffs_bs_sfs)\n",
        "            print(\"Entropy SFS:\", np.mean(sign_entropies_sfs))\n",
        "            print(\"rmse SFS:\", rmse_sfs)\n",
        "\n",
        "            # SBS\n",
        "            selected_indices_sbs, rmse_sbs, sign_entropies_sbs, coeffs_bs_sbs = evaluate_feature_selection_method(\n",
        "                sbs_select_features, X_train, y_train, X_test, y_test,\n",
        "                selector_params={'estimator': model, 'evaluator': model,\n",
        "                                'n_features_to_select': k},\n",
        "                model_params=model_params, eval_indices_bs=indices_bs)\n",
        "            # selected_features_dict[f'sbs_{i}'] = selected_indices_sbs\n",
        "            # rmse_dict[f'sbs_{i}'] = rmse_sbs\n",
        "            # entropy_dict[f'sbs_{i}'] = sign_entropies_sbs\n",
        "            selected_features_dict.setdefault('sbs', []).append(selected_indices_sbs)\n",
        "            rmse_dict.setdefault('sbs', []).append(rmse_sbs)\n",
        "            entropy_dict.setdefault('sbs', []).append(sign_entropies_sbs)\n",
        "            coeffs_dict.setdefault('sbs', []).append(coeffs_bs_sbs)\n",
        "            print(\"Entropy SBS:\", np.mean(sign_entropies_sbs))\n",
        "            print(\"rmse SBS:\", rmse_sbs)\n",
        "\n",
        "            # Bidirectional\n",
        "            selected_indices_bidirectional, rmse_bidirectional, sign_entropies_bidirectional, coeffs_bs_bidirectional = evaluate_feature_selection_method(\n",
        "                bidirectional_select_features, X_train, y_train, X_test, y_test,\n",
        "                selector_params={'estimator': model, 'evaluator': model,\n",
        "                                'n_features_to_select': k},\n",
        "                model_params=model_params, eval_indices_bs=indices_bs)\n",
        "            # selected_features_dict[f'bidirectional_{i}'] = selected_indices_bidirectional\n",
        "            # rmse_dict[f'bidirectional_{i}'] = rmse_bidirectional\n",
        "            # entropy_dict[f'bidirectional_{i}'] = sign_entropies_bidirectional\n",
        "            selected_features_dict.setdefault('bidirectional', []).append(selected_indices_bidirectional)\n",
        "            rmse_dict.setdefault('bidirectional', []).append(rmse_bidirectional)\n",
        "            entropy_dict.setdefault('bidirectional', []).append(sign_entropies_bidirectional)\n",
        "            coeffs_dict.setdefault('bidirectional', []).append(coeffs_bs_bidirectional)\n",
        "            print(\"Entropy Bidirectional:\", np.mean(sign_entropies_bidirectional))\n",
        "            print(\"rmse bidir:\", rmse_bidirectional)\n",
        "\n",
        "            print(\"##############################\")\n",
        "\n",
        "    res_dict = {'selected_features': selected_features_dict,\n",
        "                'rmse': rmse_dict,\n",
        "                'entropy': entropy_dict,\n",
        "                'coeffs': coeffs_dict\n",
        "                }\n",
        "\n",
        "\n",
        "    print(pkl_filename)\n",
        "    with open(pkl_filename, 'wb') as f1:\n",
        "        pickle.dump(res_dict, f1)\n",
        "\n",
        "    del selected_features_dict, rmse_dict, entropy_dict, coeffs_dict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}